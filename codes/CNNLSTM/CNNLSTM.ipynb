{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch torchvision h5py xarray matplotlib netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import h5py\n",
    "import PIL.Image\n",
    "from IPython.core import display as idisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from vae import VAE, vae_loss_fn\n",
    "sys.path.append('../common')\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.set_memory_limit_if_not_limit(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('save', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "pardir = os.path.dirname(os.path.dirname(cwd))\n",
    "data_folder = os.path.join(pardir,'data')\n",
    "data_path = os.path.join(data_folder,'video_prediction_dataset.hdf5')\n",
    "mask_path = os.path.join('assets','mask_black_skygptdata.png')\n",
    "model_name = 'CNNVAE'\n",
    "output_path = os.path.join(cwd,\"save\", f\"{model_name}.torch\")\n",
    "with h5py.File(data_path, 'r') as fds:\n",
    "    group_names = list(fds.keys())\n",
    "    print(group_names)\n",
    "\n",
    "dss = {}\n",
    "for gname in group_names:\n",
    "    dss[gname] = xr.open_dataset(data_path, group=gname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dss['test']['images_log'])\n",
    "print(dss['test']['images_log'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_png = np.array(PIL.Image.open(mask_path).resize((64, 64)).getdata())\n",
    "# Mask for size 64\n",
    "print(mask_png.shape)\n",
    "mask_to_black = (mask_png[:,3] > 127)\n",
    "not_mask_to_black = (mask_png[:,3] <= 127)\n",
    "print(mask_to_black.sum()) # alpha = 1 = black visible\n",
    "print(not_mask_to_black.sum())\n",
    "print(not_mask_to_black.sum()+mask_to_black.sum())\n",
    "mask_to_black = mask_to_black.reshape((64, 64))\n",
    "empty_mask = np.ones((64, 64, 3))\n",
    "mask_to_black = np.where(np.expand_dims(mask_to_black, 2), empty_mask, empty_mask*0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256 # batch size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: np.where(mask_to_black, x*0, x)),\n",
    "    #v2.functional.adjust_contrast(),\n",
    "    transforms.ToPILImage(), # This already normalizes the image\n",
    "    transforms.Lambda(v2.functional.autocontrast),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(lambda x: x.float()),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])  # Example normalization\n",
    "])\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = common.VideoDataset(dss['trainval']['images_log'], dss['trainval']['images_pred'], transform=transform, stack_videos=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_dataset = common.VideoDataset(dss['test']['images_log'], dss['test']['images_pred'], transform=transform, stack_videos=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "print(f\"Number of videos: {len(train_dataset.videos)}.\")\n",
    "print(f\"Number of video batches: {len(train_loader)}\")\n",
    "print(f\"Size of video batches: {bs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x = next(iter(train_loader))\n",
    "print(fixed_x[0].shape) # 0 because VideoDataset has batch_size videos, not images.\n",
    "print(f\"Number of images per video: {fixed_x[0].shape[0]}\")\n",
    "print(f\"Number of images per batch: {fixed_x[0].shape[0]*bs}\")\n",
    "torchvision.utils.save_image(fixed_x[0], 'save/real_image.png')\n",
    "idisplay.Image('save/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = torch.nn.Conv2d()\n",
    "#rnn = torch.nn.LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae = VAE(image_channels=image_channels).to(device)\n",
    "vae = VAE(image_channels=image_channels, z_dim=z_dim).to(device)\n",
    "last_epoch = 0\n",
    "if os.path.exists(output_path):\n",
    "    vae.load_state_dict(torch.load(output_path, map_location='cpu'))\n",
    "    last_epoch = 8\n",
    "else:\n",
    "    print('No states loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(vae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(last_epoch, epochs):\n",
    "    vae.train()\n",
    "    for idx, videos in enumerate(train_loader):\n",
    "        images = torch.stack([img[randint(0, len(img)-1)] for img in videos])\n",
    "        # Only taking the first into account, all images of videos are very similar\n",
    "        #images = videos.flatten(0,1)\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, bce, kld = vae_loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 10 == 0:\n",
    "            to_print = (\n",
    "                f\"Epoch[{epoch+1}/{epochs}] B[{idx+1}/{len(train_loader)}] Loss: {loss.data.item()/bs:.4g} \"\n",
    "                f\"{bce.data.item()/bs:.4g} {kld.data.item()/bs:.3g}\"\n",
    "            )\n",
    "            print(to_print)\n",
    "    torch.save(vae.state_dict(), output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = vae(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data):\n",
    "    torchvision.utils.save_image(data, 'save/sample_image.png')\n",
    "    display(idisplay.Image('save/sample_image.png', width=700, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "fixed_x = train_dataset[randint(1, 5000)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "show(compare_x.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "fixed_x = test_dataset[randint(0, 4466)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "show(compare_x.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, cnnvae: VAE, z_dim, hidden_dim, num_layers=1):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.cnnvae = cnnvae\n",
    "        self.lstm_enc = nn.LSTM(z_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        #input_size??\n",
    "        input_size=z_dim\n",
    "        self.lstm_dec = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "\n",
    "    def forward(self, video, target_len: int):\n",
    "        seq_dims = video.shape[0:2]\n",
    "        video = video.flatten(0,1)\n",
    "        zvects, _, _ = self.cnnvae.encode(video)\n",
    "        zvects = torch.unflatten(zvects, 0, seq_dims)\n",
    "        ## LSTM\n",
    "        h_0 = torch.zeros(self.num_layers, zvects.size(0), self.hidden_dim).to(zvects.device)  # Initialize hidden state\n",
    "        c_0 = torch.zeros(self.num_layers, zvects.size(0), self.hidden_dim).to(zvects.device)  # Initialize cell state\n",
    "        _, (h, c) = self.lstm_enc(zvects, (h_0, c_0))\n",
    "        batch_size = zvects.size(0)\n",
    "        target_size = zvects.size(2)\n",
    "        lstm_dec_input = torch.zeros((batch_size, 1, target_size)).to(zvects.device)\n",
    "        lstm_out = torch.zeros((batch_size, target_len, target_size)).to(zvects.device)\n",
    "        for t in range(target_len):\n",
    "            out, (h, c) = self.lstm_dec(lstm_dec_input, (h, c))\n",
    "            out = self.fc(out)\n",
    "            lstm_out[:, t, :] = out.squeeze(1)\n",
    "            lstm_dec_input = out\n",
    "        #lstm_out = self.fc(lstm_out)\n",
    "        lstm_out = lstm_out.flatten(0,1)\n",
    "        ##\n",
    "        predicted = self.cnnvae.decode(lstm_out)\n",
    "        predicted = torch.unflatten(predicted, 0, (seq_dims[0], target_len))\n",
    "        return predicted\n",
    "\n",
    "    def parameters(self):\n",
    "        return itertools.chain(self.lstm_enc.parameters(), self.lstm_dec.parameters(), self.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNNLSTM'\n",
    "output_path = os.path.join(cwd,\"save\", f\"{model_name}.torch\")\n",
    "\n",
    "cnnvae = vae\n",
    "cnnvae.eval()\n",
    "lstm = CNNLSTM(cnnvae, z_dim, 128, 5)\n",
    "\n",
    "\n",
    "last_epoch = 0\n",
    "if os.path.exists(output_path):\n",
    "    lstm.load_state_dict(torch.load(output_path, map_location='cpu'))\n",
    "    last_epoch = 1\n",
    "else:\n",
    "    print('No states loaded')\n",
    "\n",
    "for param in lstm.cnnvae.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "# Recreate Dataset and DataLoader, now without stacking\n",
    "train_dataset = common.VideoDataset(dss['trainval']['images_log'], dss['trainval']['images_pred'], transform=transform, stack_videos=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_dataset = common.VideoDataset(dss['test']['images_log'], dss['test']['images_pred'], transform=transform, stack_videos=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_video, target_video = train_dataset[randint(1, 5000)]\n",
    "    target_fuzzy, _, _ = lstm.cnnvae(target_video)\n",
    "    pred = lstm(input_video.unsqueeze(0), 15).squeeze(0)\n",
    "    show(pred.data.cpu())\n",
    "    show(target_fuzzy.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = lambda o,t: F.binary_cross_entropy(o,t, reduction='sum')#NLLLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-3) \n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=0.1)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    lstm.train()\n",
    "    lstm.cnnvae.eval()\n",
    "    for idx, (videos, targets) in enumerate(train_loader):\n",
    "        #lstm.lstm.zero_grad()\n",
    "        outputs = lstm(videos, targets.size(1))\n",
    "        seq_dims = targets.shape[0:2]\n",
    "        targets = targets.flatten(0,1)\n",
    "        targets, _, _ = lstm.cnnvae(targets)\n",
    "        targets = torch.unflatten(targets, 0, seq_dims)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 10 == 0:\n",
    "            to_print = (\n",
    "                f\"Epoch[{epoch+1}/{epochs}] B[{idx+1}/{len(train_loader)}]\"\n",
    "                f\" Loss: {loss.data.item()/bs:.4g} \"\n",
    "            )\n",
    "            print(to_print)\n",
    "    torch.save(vae.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_video, target_video = train_dataset[randint(1, 5000)]\n",
    "    target_fuzzy, _, _ = lstm.cnnvae(target_video)\n",
    "    pred = lstm(input_video.unsqueeze(0), 15).squeeze(0)\n",
    "    show(pred.data.cpu())\n",
    "    show(target_fuzzy.data.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
