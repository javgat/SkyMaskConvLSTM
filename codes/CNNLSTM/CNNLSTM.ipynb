{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch torchvision h5py xarray matplotlib netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import h5py\n",
    "import PIL\n",
    "from IPython.core import display as idisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from vae import VAE, vae_loss_fn\n",
    "sys.path.append('../common')\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.set_memory_limit_if_not_limit(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('save', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "pardir = os.path.dirname(os.path.dirname(cwd))\n",
    "data_folder = os.path.join(pardir,'data')\n",
    "data_path = os.path.join(data_folder,'video_prediction_dataset.hdf5')\n",
    "model_name = 'CNNLSTM'\n",
    "output_path = os.path.join(cwd,\"save\", f\"{model_name}.torch\")\n",
    "with h5py.File(data_path, 'r') as fds:\n",
    "    group_names = list(fds.keys())\n",
    "    print(group_names)\n",
    "\n",
    "dss = {}\n",
    "for gname in group_names:\n",
    "    dss[gname] = xr.open_dataset(data_path, group=gname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dss['test']['images_log'])\n",
    "print(dss['test']['images_log'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64 # batch size\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # This already normalizes the image\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(lambda x: x.float()),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])  # Example normalization\n",
    "])\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = common.VideoDataset(dss['trainval']['images_log'], dss['trainval']['images_pred'], transform=transform, stack_videos=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "test_dataset = common.VideoDataset(dss['test']['images_log'], dss['test']['images_pred'], transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "print(f\"Number of videos: {len(train_dataset.videos)}.\")\n",
    "print(f\"Number of video batches: {len(train_loader)}\")\n",
    "print(f\"Size of video batches: {bs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x = next(iter(train_loader))\n",
    "print(fixed_x[0].shape) # 0 because VideoDataset has batch_size videos, not images.\n",
    "print(f\"Number of images per video: {fixed_x[0].shape[0]}\")\n",
    "print(f\"Number of images per batch: {fixed_x[0].shape[0]*bs}\")\n",
    "torchvision.utils.save_image(fixed_x[0], 'save/real_image.png')\n",
    "idisplay.Image('save/real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = torch.nn.Conv2d()\n",
    "#rnn = torch.nn.LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = fixed_x[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(image_channels=image_channels).to(device)\n",
    "if os.path.exists(output_path):\n",
    "    vae.load_state_dict(torch.load(output_path, map_location='cpu'))\n",
    "else:\n",
    "    print('No states loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    for idx, (videos) in enumerate(train_loader):\n",
    "        images = torch.stack([img[randint(0, len(img)-1)] for img in videos])\n",
    "        # Only taking the first into account, all images of videos are very similar\n",
    "        #images = videos.flatten(0,1)\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, bce, kld = vae_loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        to_print = (\n",
    "            f\"Epoch[{epoch+1}/{epochs}] B[{idx+1}/{len(train_loader)}] Loss: {loss.data.item()/bs:.4g} \"\n",
    "            f\"{bce.data.item()/bs:.4g} {kld.data.item()/bs:.3g}\"\n",
    "        )\n",
    "        print(to_print)\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = vae(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "fixed_x = train_dataset[randint(1, 100)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x)\n",
    "\n",
    "torchvision.utils.save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(idisplay.Image('sample_image.png', width=700, unconfined=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
