{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3db3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f063ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db217fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  COLAB_BASE_DIR = \"/content/gdrive/MyDrive/Universidad/VIU/TFM/notebooks/\"\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive/')\n",
    "  sys.path.append(os.path.join(COLAB_BASE_DIR, 'SkyGPT'))\n",
    "  sys.path.append(os.path.join(COLAB_BASE_DIR, 'common'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b355b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  if not os.path.exists(\"/data/video_prediction_dataset.hdf5\"):\n",
    "    !gdown 1fYtaFcGKSL8ykJFbewAFsFf9C4aW0DuV\n",
    "    !gdown 1VILdkCRWsDTrN9DPeMLh8jlibAoBLzy-\n",
    "    !gdown 197pDAI8KVsiDAA1xaPbitZpmvzh9CDqT\n",
    "    !gdown 1XqgXAtxWUdnBJnvQTtEr8JpBkVi3JuSC\n",
    "    !gdown 1_DVttDvwxCm_QGDxU2tbaaclWKd1VCau\n",
    "    !gdown 1QkqsqjYhdJbqf2HUOQXwroYaO0uOLUPm\n",
    "    %mkdir -p /data\n",
    "    %mv /content/*.hdf5 /data/\n",
    "    %mv /content/*.npy /data/\n",
    "    %mv /content/*.png /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torchvision.datasets.video_utils import VideoClips\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# from videogpt import VideoData, VideoGPT, load_videogpt\n",
    "from videogpt import VideoGPT, load_videogpt\n",
    "from videogpt.utils import save_video_grid\n",
    "\n",
    "if not IN_COLAB:\n",
    "  sys.path.append('../common')\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "pardir = os.path.dirname(os.path.dirname(cwd))\n",
    "data_folder = os.path.join(pardir,'data')\n",
    "data_path = os.path.join(data_folder,'video_prediction_dataset.hdf5')\n",
    "mask_path = os.path.join(data_folder,'mask_black_skygptdata.png')\n",
    "model_name = 'ConvLSTM'\n",
    "output_path = os.path.join(cwd, \"save\", f\"{model_name}.torch\")\n",
    "IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  output_path = os.path.join(COLAB_BASE_DIR, \"ConvLSTM\", f\"{model_name}.torch\")\n",
    "with h5py.File(data_path, 'r') as fds:\n",
    "    group_names = list(fds.keys())\n",
    "    print(group_names)\n",
    "\n",
    "dss = {}\n",
    "for gname in group_names:\n",
    "    dss[gname] = xr.open_dataset(data_path, group=gname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNP(tensor, idx):\n",
    "    output = tensor[idx,:,:,:,:]\n",
    "    output = output.permute(1,2,3,0)\n",
    "    output = output.cpu()\n",
    "    return output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(video, resolution, sequence_length=None):\n",
    "    # video: THWC, {0, ..., 255}\n",
    "    video = video.permute(0, 3, 1, 2).float() / 255. # TCHW\n",
    "    t, c, h, w = video.shape\n",
    "\n",
    "    # temporal crop\n",
    "    if sequence_length is not None:\n",
    "        assert sequence_length <= t\n",
    "        video = video[:sequence_length]\n",
    "\n",
    "    # scale shorter side to resolution\n",
    "    scale = resolution / min(h, w)\n",
    "    if h < w:\n",
    "        target_size = (resolution, math.ceil(w * scale))\n",
    "    else:\n",
    "        target_size = (math.ceil(h * scale), resolution)\n",
    "    video = F.interpolate(video, size=target_size, mode='bilinear',\n",
    "                          align_corners=False)\n",
    "\n",
    "    # center crop\n",
    "    t, c, h, w = video.shape\n",
    "    w_start = (w - resolution) // 2\n",
    "    h_start = (h - resolution) // 2\n",
    "    video = video[:, :, h_start:h_start + resolution, w_start:w_start + resolution]\n",
    "    video = video.permute(1, 0, 2, 3).contiguous() # CTHW\n",
    "\n",
    "    video -= 0.5\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "class HDF5Dataset:\n",
    "    \"\"\" Generic dataset for data stored in h5py as uint8 numpy arrays.\n",
    "    Reads videos in {0, ..., 255} and returns in range [-0.5, 0.5] \"\"\"\n",
    "    def __init__(self, data_file, sequence_length, train=False, resolution=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file: path to the pickled data file with the\n",
    "                following format:\n",
    "                {\n",
    "                    'train_data': [B, H, W, 3] np.uint8,\n",
    "                    'train_idx': [B], np.int64 (start indexes for each video)\n",
    "                    'test_data': [B', H, W, 3] np.uint8,\n",
    "                    'test_idx': [B'], np.int64\n",
    "                }\n",
    "            sequence_length: length of extracted video sequences\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.sequence_length = sequence_length\n",
    "        self.resolution = resolution\n",
    "\n",
    "        # read in data\n",
    "        self.data_file = data_file\n",
    "        self.data = h5py.File(data_file, 'r')\n",
    "        self.prefix = 'train' if train else 'test'\n",
    "        self._images = self.data[f'{self.prefix}_data']\n",
    "        self._idx = self.data[f'{self.prefix}_idx']\n",
    "        self.size = len(self._idx)\n",
    "\n",
    "    def setstate(self, state):\n",
    "        self.__dict__ = state\n",
    "        self.data = h5py.File(self.data_file, 'r')\n",
    "        self._images = self.data[f'{self.prefix}_data']\n",
    "        self._idx = self.data[f'{self.prefix}_idx']\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        start = self._idx[idx]\n",
    "        end = self._idx[idx + 1] if idx < len(self._idx) - 1 else len(self._images)\n",
    "        assert end - start >= 0\n",
    "        start = start + np.random.randint(low=0, high=end - start - self.sequence_length + 1) #Andea added +1 for inclusive of endpoint\n",
    "        assert start < start + self.sequence_length <= end\n",
    "        video = torch.tensor(self._images[start:start + self.sequence_length])\n",
    "        return dict(video=preprocess(video, self.resolution), idx=idx)\n",
    "#         return dict(video=preprocess(video, self.resolution))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ccdc926",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSample(ckpt_path, n, start, data_file):\n",
    "    \n",
    "    batch = {}\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        gpt = load_videogpt(ckpt_path)\n",
    "    else:\n",
    "        gpt = VideoGPT.load_from_checkpoint(ckpt_path)\n",
    "    #gpt = gpt.cuda()\n",
    "    gpt.eval()\n",
    "\n",
    "    sequence_length = 16\n",
    "    train=False\n",
    "    resolution=64\n",
    "\n",
    "\n",
    "    data = HDF5Dataset(data_file, sequence_length, train, resolution)\n",
    "    tempBatch = torch.empty((n, 3, 16, 64, 64), device = 'cuda')\n",
    "\n",
    "    for pos, example in enumerate(range(start, start+n)):\n",
    "        item = data.getitem(example)\n",
    "        tempBatch[pos] = item['video']\n",
    "\n",
    "    batch['video'] = tempBatch\n",
    "\n",
    "    samples = gpt.sample(n, batch)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f485acb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runIteration(ckpt_path, num_test, n, it, save_folder, data_file):\n",
    "    all_test_samples = np.empty([num_test, 16, 64, 64, 3]) #initialize a holder for the sample results (diff from sample shape)\n",
    "\n",
    "    step = n\n",
    "    for batch_start in range(0, num_test, step):\n",
    "\n",
    "        start = batch_start\n",
    "\n",
    "        #check for partial batch at end\n",
    "        if (start + step) > num_test:\n",
    "            n = num_test - start\n",
    "\n",
    "        output = getSample(ckpt_path, n, start, data_file)\n",
    "        output = output.permute(0,2,3,4,1) #original shape is (num_examples, 3, 16, 64, 64)\n",
    "        output = output.cpu()\n",
    "        output = output.numpy()\n",
    "        all_test_samples[start:(start+n)] = output\n",
    "    filename = save_folder + \"/sample_\" + str(it) + \".npy\"\n",
    "    np.save(filename, all_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f1eea9d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test = 2582 # number of samples in the dataset\n",
    "#num_test = 2\n",
    "n = 50 # num of futures to be generated for each sample\n",
    "#ckpt_path = '/scratch/groups/abrandt/solar_forecasting/GAN_project_new/models/VideoGPT/trained_models/PhyGPT_full_2min/lightning_logs/version_36584693/checkpoints/epoch=17-step=30005.ckpt'\n",
    "ckpt_path = 'ucf101_uncond_gpt'\n",
    "#data_file = 'GPT_full_2min_2019_test_data.hdf5'\n",
    "#data_file = 'GPT_benchmark.hdf5'\n",
    "data_file = '../../data/video_prediction_dataset.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2f913b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    }
   ],
   "source": [
    "#save_folder = \"/scratch/groups/abrandt/solar_forecasting/GAN_project_new/models/PhyGPT/inference/PhyGPT4x4x4_2019_test\"\n",
    "save_folder = \"save\"\n",
    "if os.path.isdir(save_folder)==False:\n",
    "    os.makedirs(save_folder)\n",
    "print(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d4d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javgat/VIU/TFM/presente/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.3 to v2.3.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../.cache/videogpt/ucf101_uncond_gpt`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.1 to v2.3.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../.cache/videogpt/ucf101_stride4x4x4`\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'test_data' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_it \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m45\u001b[39m, num_it):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrunIteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;124m'\u001b[39m, it, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mrunIteration\u001b[0;34m(ckpt_path, num_test, n, it, save_folder, data_file)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (start \u001b[38;5;241m+\u001b[39m step) \u001b[38;5;241m>\u001b[39m num_test:\n\u001b[1;32m     11\u001b[0m     n \u001b[38;5;241m=\u001b[39m num_test \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgetSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#original shape is (num_examples, 3, 16, 64, 64)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mgetSample\u001b[0;34m(ckpt_path, n, start, data_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[0;32m---> 16\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mHDF5Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m tempBatch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((n, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(start, start\u001b[38;5;241m+\u001b[39mn)):\n",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m, in \u001b[0;36mHDF5Dataset.__init__\u001b[0;34m(self, data_file, sequence_length, train, resolution)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(data_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/VIU/TFM/presente/.venv/lib/python3.8/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:241\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'test_data' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "num_it = 50\n",
    "for it in range(45, num_it):\n",
    "    runIteration(ckpt_path, num_test, n, it, save_folder, data_file)\n",
    "    print('Iteration ', it, 'complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
